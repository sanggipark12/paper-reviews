{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "act_dict = {}\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        act_dict[name] = input[0].detach().cpu()\n",
    "    return hook\n",
    "\n",
    "layer_name = 'decoder.layers.0.self_attn.q_proj2'\n",
    "target_layer = model.model.decoder.layers[0].self_attn.q_proj\n",
    "handle = target_layer.register_forward_hook(get_activation_hook(layer_name))\n",
    "\n",
    "input_text = \"SmoothQuant is a training-free, accuracy-preserving solution.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "model(**inputs)\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "\n",
    "activation = act_dict[layer_name] # Shape [Batch, Seq_Len, Hidden_Dim]\n",
    "\n",
    "weight = target_layer.weight.detach().cpu() # Shape [Out_Dim, In_Dim]\n",
    "\n",
    "print(f\"Activation Shape: {activation.shape}\")\n",
    "print(f\"Weight Shape: {weight.shape}\")\n",
    "\n",
    "# Activation의 전체 값 중 최댓값\n",
    "print(f\"Activation Max Abs: {activation.abs().max().item()}\")\n",
    "\n",
    "# Weight의 전체 값 중 최댓값\n",
    "print(f\"Weight Max Abs: {weight.abs().max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_smoothing_factor(activation, weight, alpha=0.5):\n",
    "    # Activation의 채널별 절댓값 최댓값 구하기 \n",
    "    act_abs = activation.abs()\n",
    "    act_scales = act_abs.view(-1, activation.shape[-1]).max(dim=0)[0]\n",
    "\n",
    "    # Weight의 채널별 절댓값 최댓값 구하기 \n",
    "    weight_abs = weight.abs()\n",
    "    weight_scales = weight_abs.max(dim=0)[0]\n",
    "\n",
    "    # 0으로 나누는 것을 방지\n",
    "    weight_scales = torch.maximum(weight_scales, torch.tensor(1e-5))\n",
    "\n",
    "    # s = (act_scales ^ alpha) / (weight_scales ^ (1 - alpha))\n",
    "    scales = torch.pow(act_scales, alpha) / torch.pow(weight_scales, (1 - alpha))\n",
    "\n",
    "    return scales\n",
    "\n",
    "# 실행 및 확인\n",
    "scales = calculate_smoothing_factor(activation, weight)\n",
    "\n",
    "print(f\"Scales Shape: {scales.shape}\") \n",
    "print(f\"Calculated Scales (First 5): {scales[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Activation Smoothing (나누기)\n",
    "# X_hat = X / s\n",
    "act_s = scales.view(1,1,-1)\n",
    "smoothed_act = activation / act_s\n",
    "\n",
    "# 2. Weight Smoothing (곱하기)\n",
    "# W_hat = W * s\n",
    "w_s = scales.view(1, -1)\n",
    "smoothed_weight = weight * w_s\n",
    "\n",
    "# --- 검증 ---\n",
    "print(f\"Original Activation Max: {activation.abs().max().item():.2f}\")\n",
    "print(f\"Smoothed Activation Max: {smoothed_act.abs().max().item():.2f}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Original Weight Max: {weight.abs().max().item():.4f}\")\n",
    "print(f\"Smoothed Weight Max: {smoothed_weight.abs().max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def fake_quantize(tensor):\n",
    "    scale = tensor.abs().max() / 127.0\n",
    "\n",
    "    quant_int8 = torch.clamp(torch.round(tensor / scale) , -128, 127)\n",
    "\n",
    "    recon_tensor = scale * quant_int8\n",
    "\n",
    "    return recon_tensor\n",
    "\n",
    "output_gt = torch.matmul(activation, weight.t())\n",
    "\n",
    "# 원래 값 양자화\n",
    "act_naive_q = fake_quantize(activation)\n",
    "w_naive_q = fake_quantize(weight)\n",
    "output_naive = torch.matmul(act_naive_q, w_naive_q.t())\n",
    "\n",
    "# 스무딩 양자화\n",
    "act_smooth_q = fake_quantize(smoothed_act)\n",
    "w_smooth_q = fake_quantize(smoothed_weight)\n",
    "output_smooth = torch.matmul(act_smooth_q, w_smooth_q.t())\n",
    "\n",
    "# 오차 비교\n",
    "loss_naive = (output_gt - output_naive).abs().mean().item()\n",
    "loss_smooth = (output_gt - output_smooth).abs().mean().item()\n",
    "\n",
    "print(f\"Naive Quant Error:   {loss_naive:.6f}\")\n",
    "print(f\"SmoothQuant Error:   {loss_smooth:.6f}\")\n",
    "\n",
    "if loss_smooth < loss_naive:\n",
    "    print(\"SmoothQuant가 오차 줄임\")\n",
    "else:\n",
    "    print(\"조정이 필요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_distribution(original, smoothed):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 1. 원본 Activation 분포\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(original.view(-1).numpy(), bins=100, color='red', alpha=0.6)\n",
    "    plt.title(f\"Original Activation\\nMax: {original.abs().max():.2f}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.yscale('log') # 로그 스케일로 봐야 Outlier가 잘 보입니다\n",
    "\n",
    "    # 2. SmoothQuant 적용 후 분포\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(smoothed.view(-1).numpy(), bins=100, color='green', alpha=0.6)\n",
    "    plt.title(f\"Smoothed Activation\\nMax: {smoothed.abs().max():.2f}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 시각화 실행\n",
    "# (데이터가 너무 많으면 느릴 수 있으니 일부만 샘플링해서 그립니다)\n",
    "plot_distribution(activation[0], smoothed_act[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
