{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acc255",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e070e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "act_list = []\n",
    "\n",
    "def hook(model, input, output):\n",
    "    act_list.append(input[0].detach().cpu())\n",
    "\n",
    "\n",
    "layer_name = \"decoder.layers.0.self_attn.q_proj2\"\n",
    "target_layer = model.model.decoder.layers[0].self_attn.q_proj\n",
    "handle = target_layer.register_forward_hook(hook)\n",
    "\n",
    "calibration_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world at an unprecedented pace.\",\n",
    "    \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods.\",\n",
    "    \"To be, or not to be, that is the question.\",\n",
    "    \"The capital of France is Paris, which is known for its cafe culture and landmarks.\",\n",
    "    \"Deep learning models require vast amounts of data to train effectively.\",\n",
    "    \"The theory of relativity was developed by Albert Einstein in the early 20th century.\",\n",
    "    \"Python is a high-level, interpreted programming language known for its readability.\",\n",
    "    \"Global warming is the long-term heating of Earth's climate system observed since the pre-industrial period.\",\n",
    "    \"In 1492, Christopher Columbus sailed the ocean blue.\",\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"Machine learning algorithms build a model based on sample data, known as training data.\",\n",
    "    \"The Great Wall of China is a series of fortifications that were built across the historical northern borders of China.\",\n",
    "    \"Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms.\",\n",
    "    \"I wandered lonely as a cloud that floats on high o'er vales and hills.\",\n",
    "    \"The internet is a global system of interconnected computer networks.\",\n",
    "    \"Water boils at 100 degrees Celsius at standard atmospheric pressure.\",\n",
    "    \"The history of computers dates back to the invention of the abacus.\",\n",
    "    \"Music is an art form, and cultural activity, whose medium is sound.\",\n",
    "    \"Democracy is a form of government in which the people have the authority to deliberate and decide legislation.\",\n",
    "    \"The human brain is the central organ of the human nervous system.\",\n",
    "    \"Space exploration helps us understand the universe and our place in it.\",\n",
    "    \"Renewable energy is energy that is collected from renewable resources, which are naturally replenished.\",\n",
    "    \"Economics is the social science that studies the production, distribution, and consumption of goods and services.\",\n",
    "    \"Literature broadly is any collection of written work, but it is also used more narrowly for writings specifically considered to be an art form.\",\n",
    "    \"def quicksort(arr): if len(arr) <= 1: return arr\",\n",
    "    \"Linear algebra is central to almost all areas of mathematics.\",\n",
    "    \"The stock market is a collection of markets and exchanges where regular activities of buying, selling, and issuance of shares of publicly-held companies take place.\",\n",
    "    \"Coffee is a brewed drink prepared from roasted coffee beans, the seeds of berries from certain Coffea species.\",\n",
    "    \"Optimization is the selection of a best element from some set of available alternatives.\"\n",
    "]\n",
    "for text in calibration_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model(**inputs)\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "all_acts = torch.cat(act_list, dim=1).squeeze(0) # Shape: [Total Tokens, Hidden_Dim]\n",
    "print(f\"Total Tokens Collected: {all_acts.shape[0]}\")\n",
    "\n",
    "weight = target_layer.weight.detach().cpu() # Shape: [Out_Dim, In_Dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b22b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ï±ÑÎÑêÎ≥Ñ ÌèâÍ∑† Ï†àÎåÄÍ∞í Í≥ÑÏÇ∞ (s_x)\n",
    "per_channel_mean = all_acts.abs().mean(dim=0).float() # [2048]\n",
    "\n",
    "print(f\"Per Channel Mean Shape: {per_channel_mean.shape}\")\n",
    "\n",
    "# ÏÉÅÏúÑ 5Í∞ú Ïù∏Îç±Ïä§ ÏôÄ Í∞í Ï∂úÎ†•\n",
    "top_indices = per_channel_mean.argsort(descending=True)[:5]\n",
    "\n",
    "print(f\"Top Indices: {top_indices}, value : {per_channel_mean[top_indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f62ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def pseudo_quantize_grouped(w, group_size=128):\n",
    "    out_features, in_features = w.shape\n",
    "    num_groups = in_features // group_size\n",
    "\n",
    "    # [Out, In] -> [Out, Num_Groups, Group_Size]\n",
    "    w_grouped = w.view(out_features, num_groups, group_size)\n",
    "\n",
    "    # Max value per group\n",
    "    max_val = w_grouped.abs().amax(dim=-1, keepdim=True)\n",
    "    scale = max_val / 7 + 1e-5\n",
    "\n",
    "    # Quantize & Dequantize\n",
    "    w_int = torch.round(w_grouped / scale)\n",
    "    w_int = torch.clamp(w_int, min=-7, max=7)\n",
    "    w_dequant = w_int * scale\n",
    "\n",
    "    return w_dequant.view(out_features, in_features)\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "x = all_acts.to(weight.device).float()\n",
    "w_float = weight.float()\n",
    "\n",
    "# Original Output\n",
    "y_orig = torch.matmul(x, w_float.t())\n",
    "\n",
    "# RTN Baseline (Alpha=0)\n",
    "w_rtn = pseudo_quantize_grouped(w_float, group_size=128)\n",
    "y_rtn = torch.matmul(x, w_rtn.t())\n",
    "mse_rtn = (y_orig - y_rtn).pow(2).mean().item()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Alpha':<10} | {'Output MSE':<15} | {'Improvement'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_alpha = -1\n",
    "\n",
    "for alpha in [i * 0.1 for i in range(11)]:\n",
    "    # Scale Í≥ÑÏÇ∞ (s = s_x ^ alpha)\n",
    "    current_scale = (per_channel_mean + 1e-5).pow(alpha).to(weight.device)\n",
    "\n",
    "    # AWQ Ï†ÅÏö©: Scale -> Quantize -> Inverse Scale\n",
    "    w_scaled = w_float * current_scale\n",
    "    w_q_scaled = pseudo_quantize_grouped(w_scaled, group_size=128)\n",
    "    w_restored = w_q_scaled / current_scale\n",
    "\n",
    "    # MSE Í≥ÑÏÇ∞\n",
    "    y_awq = torch.matmul(x, w_restored.t())\n",
    "    mse = (y_orig - y_awq).pow(2).mean().item()\n",
    "\n",
    "    # Improvement ÌôïÏù∏ (RTN ÎåÄÎπÑ ÏñºÎßàÎÇò Ï§ÑÏóàÎÇò)\n",
    "    imp_str = \"Better! üéâ\" if mse < mse_rtn else \"Worse\"\n",
    "    print(f\"{alpha:<10.1f} | {mse:<15.6f} | {imp_str}\")\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Original RTN MSE : {mse_rtn:.6f}\")\n",
    "print(f\"Best AWQ MSE     : {best_mse:.6f} (at alpha={best_alpha:.1f})\")\n",
    "\n",
    "if best_mse < mse_rtn:\n",
    "    print(f\"ÏÑ±Í≥µ! \")\n",
    "else:\n",
    "    print(\"Ï°∞Ï†ï ÌïÑÏöî\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
