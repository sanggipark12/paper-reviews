{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xcEGUNodFho"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaRoundQuantizer(nn.Module):\n",
        "    def __init__(self, weight, scale, delta=1.1, gamma=-0.1):\n",
        "        super().__init__()\n",
        "        # delta, gamma : 시그모이드 극값 조절\n",
        "        # 극값을 늘려서 미분이 0이 되지 않게 계속 학습 되도록\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.scale = scale\n",
        "\n",
        "        # V 초기화 (학습 파라미터)\n",
        "        # FP32 가중치의 소수점 값으로 초기화\n",
        "        # h(v) = rest의 역함수를 사용\n",
        "        w_floor = torch.floor(weight / scale)\n",
        "        rest = (weight / scale) - w_floor\n",
        "\n",
        "        rest = torch.clamp(rest, 0.01, 0.99)\n",
        "\n",
        "        sig_inv = (rest - gamma) / (delta - gamma)\n",
        "        sig_inv = torch.clamp(sig_inv, 0.01, 0.99)\n",
        "\n",
        "        init_v = torch.log(sig_inv / (1 - sig_inv))\n",
        "\n",
        "        self.V = nn.Parameter(init_v)\n",
        "\n",
        "    def rectified_sigmoid(self):\n",
        "        # Equation 23\n",
        "        x = torch.clamp(torch.sigmoid(self.V) * (self.delta - self.gamma) + self.gamma, 0, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, weight, scale, zero_points=0):\n",
        "\n",
        "        w_floor = torch.floor(weight / scale + zero_points)\n",
        "\n",
        "        # 반올림 결정 부분\n",
        "        w_soft = w_floor + self.rectified_sigmoid()\n",
        "        # 역양자화\n",
        "        w_dequant = scale * (w_soft - zero_points)\n",
        "\n",
        "        return w_dequant"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaRoundLoss(nn.Module):\n",
        "    def __init__(self, weight_decay=1e-4):\n",
        "        super().__init__()\n",
        "        self.weight_decay = weight_decay # lambda\n",
        "\n",
        "    def compute_reg_loss(self, h_V, beta):\n",
        "        # 2 * h_V - 1 : 0.5 보다 커지면 1에 가까워진다.\n",
        "        # beta가 크면 1보다 작은 경우 0에 확 가까워짐\n",
        "        reg_loss = torch.sum(1 - torch.pow(torch.abs(2 * h_V - 1), beta))\n",
        "        return reg_loss\n",
        "\n",
        "    def forward(self, current_out, orig_out, h_V, beta):\n",
        "\n",
        "        mse_loss = F.mse_loss(current_out, orig_out)\n",
        "\n",
        "        reg_loss = self.compute_reg_loss(h_V, beta)\n",
        "\n",
        "        total_loss = mse_loss + self.weight_decay * reg_loss\n",
        "        # mse니깐 L2 정규화를 생각해보면 보통 람다를 0.9를 주는데 어떤 비율로 조정해야할까?\n",
        "        # 논문에서는 frobenius norm을 사용한다. 이거는 제곱의 합이므로 값이 크다. 따라서 조정이 필요한데\n",
        "        # mse에 reduction = 'sum'을 해주던가 정규화 쪽을 파라미터 개수로 나누던가 람다를 매우 작게 잡는다\n",
        "        # 초반에는 mse가 중요하니깐 재구성 손실과 비슷하게 맞춘다 보통 1e-4 ~ 1e-2\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "w1jY_Dr_rL1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantization_params(weight, bits=4):\n",
        "\n",
        "    # 4-bit의 경우 범위: -8 ~ 7\n",
        "    q_max = 2**(bits - 1) - 1 # 7\n",
        "    q_min = -2**(bits - 1)    # -8\n",
        "\n",
        "    max_val = torch.max(torch.abs(weight))\n",
        "    scale = max_val / q_max\n",
        "\n",
        "    # zero-point: symmetric\n",
        "    zero_point = 0\n",
        "\n",
        "    return scale, zero_point"
      ],
      "metadata": {
        "id": "30jPNLni2NjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "\n",
        "def train_adaround(model_layer, input_data, orig_output, quantizer, n_iter=2000):\n",
        "\n",
        "    # Scale과 ZeroPoint 미리 계산\n",
        "    scale, zero_point = get_quantization_params(model_layer.weight)\n",
        "    scale = scale.to(model_layer.weight.device)\n",
        "\n",
        "    # Optimizer 설정 (Parameter V만 학습)\n",
        "    optimizer = Adam([quantizer.V], lr=1e-3)\n",
        "\n",
        "    loss_func = AdaRoundLoss(weight_decay=1e-4)\n",
        "\n",
        "    # Beta Scheduling (20 -> 2 로 감소)\n",
        "    beta_scheduler = np.linspace(20, 2, n_iter)\n",
        "\n",
        "    print(\"Start AdaRound Training...\")\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        beta = beta_scheduler[i]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Soft Quantized Weight 생성\n",
        "        # 여기서 V가 학습되면서 반올림 여부가 결정됨\n",
        "        w_soft_quant = quantizer(model_layer.weight, scale, zero_point)\n",
        "\n",
        "        # Forward Pass\n",
        "        # 원래 weight를 백업하고, quantized weight를 덮어씌워서 연산\n",
        "        saved_weight = model_layer.weight.data.clone()\n",
        "        model_layer.weight.data = w_soft_quant\n",
        "\n",
        "        # 입력 데이터를 넣어 출력 계산\n",
        "        current_out = model_layer(input_data)\n",
        "\n",
        "        # weight 복구\n",
        "        model_layer.weight.data = saved_weight\n",
        "\n",
        "        # Loss 계산\n",
        "        h_V = quantizer.rectified_sigmoid()\n",
        "        loss = loss_func(current_out, orig_output, h_V, beta)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            print(f\"Iter {i}: Loss {loss.item():.4f}, Beta {beta:.2f}\")\n",
        "\n",
        "    # Final Hard Rounding\n",
        "    with torch.no_grad():\n",
        "        # h(V)가 0.5 이상이면 1(올림), 아니면 0(내림)이 되도록 처리 필요하지만,\n",
        "        # AdaRound 논문에서는 학습된 V를 이용해 soft quantization 식을 그대로 쓰되\n",
        "        # h(V)가 거의 0 또는 1로 수렴했으므로 결과적으로 Hard Rounding이 됨.\n",
        "        # 명확하게 하기 위해 마지막엔 round()를 한 번 더 씌우기도 함.\n",
        "        final_w_quant = quantizer(model_layer.weight, scale, zero_point)\n",
        "\n",
        "    return final_w_quant"
      ],
      "metadata": {
        "id": "yboH1yNZu7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSaverHook:\n",
        "    def __init__(self, store_input=False, store_output=False):\n",
        "        self.store_input = store_input\n",
        "        self.store_output = store_output\n",
        "        self.inputs = []\n",
        "        self.outputs = []\n",
        "\n",
        "    def hook_fn(self, module, input_t, output_t):\n",
        "        if self.store_input:\n",
        "            # detach()를 해서 그래디언트 연결을 끊고, cpu()로 옮겨 메모리를 아끼는 것이 좋다고함.\n",
        "            self.inputs.append(input_t[0].detach().cpu())\n",
        "\n",
        "        if self.store_output:\n",
        "            self.outputs.append(output_t.detach().cpu())"
      ],
      "metadata": {
        "id": "OrbMPfgE2qUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_layer_inputs_and_outputs(model, target_layer, calib_dataloader, device='cuda'):\n",
        "    # Hook 등록\n",
        "    data_saver = DataSaverHook(store_input=True, store_output=True)\n",
        "    handle = target_layer.register_forward_hook(data_saver.hook_fn)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"Collecting calibration data for layer: {target_layer}...\")\n",
        "\n",
        "    # 모델 전체 Forward Pass\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in calib_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            model(inputs) #데이터를 수집함\n",
        "\n",
        "    # Hook 제거\n",
        "    handle.remove()\n",
        "\n",
        "    # 수집된 데이터를 하나의 텐서로 합치기\n",
        "    all_inputs = torch.cat(data_saver.inputs, dim=0).to(device)\n",
        "    all_outputs = torch.cat(data_saver.outputs, dim=0).to(device)\n",
        "\n",
        "    print(f\"Data Collected! Input shape: {all_inputs.shape}, Output shape: {all_outputs.shape}\")\n",
        "\n",
        "    return all_inputs, all_outputs"
      ],
      "metadata": {
        "id": "aWDDSYdTQlYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def fold_conv_bn(conv, bn):\n",
        "    \"\"\"\n",
        "    Conv2d와 BatchNorm2d를 입력받아, BN 파라미터를 Conv에 흡수(Folding)시킵니다.\n",
        "    \"\"\"\n",
        "    # BN 파라미터 가져오기\n",
        "    # running_mean, running_var, weight(gamma), bias(beta)\n",
        "    mu = bn.running_mean\n",
        "    var = bn.running_var\n",
        "    gamma = bn.weight\n",
        "    beta = bn.bias\n",
        "    eps = bn.eps\n",
        "\n",
        "    # Conv 파라미터 가져오기\n",
        "    W = conv.weight\n",
        "    b = conv.bias if conv.bias is not None else torch.zeros_like(mu)\n",
        "\n",
        "    # 분모 계산: sigma = sqrt(var + eps)\n",
        "    denom = torch.sqrt(var + eps)\n",
        "\n",
        "    # Scale Factor 계산: gamma / sigma\n",
        "    scale_factor = gamma / denom\n",
        "\n",
        "\n",
        "    # 새로운 Weight 계산: W * scale\n",
        "    # view(-1, 1, 1, 1)은 (Channel, 1, 1, 1)로 만들어 Broadcasting을 가능하게 함\n",
        "    W_new = W * scale_factor.view(-1, 1, 1, 1)\n",
        "\n",
        "    # 새로운 Bias 계산: beta + (b - mu) * scale\n",
        "    b_new = beta + (b - mu) * scale_factor\n",
        "\n",
        "    # Conv 레이어 업데이트\n",
        "    conv.weight.data.copy_(W_new)\n",
        "\n",
        "\n",
        "    # Conv에 바이어스가 없었다면 새로 만들어줘야 함\n",
        "    if conv.bias is None:\n",
        "        conv.bias = nn.Parameter(b_new)\n",
        "    else:\n",
        "        conv.bias.data.copy_(b_new)\n",
        "\n",
        "\n",
        "    # 5. BN 레이어는 이제 필요 없으므로 Identity로 대체 (아무 일도 안 하는 레이어)\n",
        "    return nn.Identity()"
      ],
      "metadata": {
        "id": "93lH0bVrguNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fuse_resnet_model(model):\n",
        "\n",
        "    model.eval() #eval 모드여야 running_mean/var가 고정됨\n",
        "\n",
        "    # 첫 번째 레이어 (Conv1 + BN1)\n",
        "    if hasattr(model, 'bn1') and not isinstance(model.bn1, nn.Identity):\n",
        "        print(\"Fusing model.conv1 and model.bn1...\")\n",
        "        model.bn1 = fold_conv_bn(model.conv1, model.bn1)\n",
        "\n",
        "\n",
        "    # ResNet Layer 1~4 내부의 BasicBlock 순회\n",
        "    # layer -> block -> (conv1+bn1), (conv2+bn2)\n",
        "    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
        "        layer = getattr(model, layer_name)\n",
        "        for i, block in enumerate(layer):\n",
        "            print(f\"Fusing {layer_name} Block {i}...\")\n",
        "\n",
        "            # Block 내 첫 번째 쌍\n",
        "            block.bn1 = fold_conv_bn(block.conv1, block.bn1)\n",
        "\n",
        "            # Block 내 두 번째 쌍\n",
        "            block.bn2 = fold_conv_bn(block.conv2, block.bn2)\n",
        "\n",
        "            # Downsample 레이어가 있는 경우 (conv+bn)\n",
        "            if block.downsample is not None:\n",
        "                # downsample[0]은 Conv, downsample[1]은 BN\n",
        "                block.downsample[1] = fold_conv_bn(block.downsample[0], block.downsample[1])\n",
        "\n",
        "\n",
        "    print(\"BN Folding Complete!\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "fQY3LSNmg04N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "q_model = copy.deepcopy(model)\n",
        "q_model.eval()\n",
        "q_model.to(deivce)\n",
        "q_model = fuse_resnet_model(q_model)\n",
        "\n",
        "# 더미 데이터\n",
        "dummy_data = torch.randn(1024, 3, 224, 224)\n",
        "dataset = torch.utils.data.TensorDataset(dummy_data, torch.zeros(1024))\n",
        "calib_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "for name, module in q_model.named_modules():\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        print(f\"Quantizer Layer : {name}\")\n",
        "\n",
        "        input_data, orig_output = get_layer_inputs_and_outputs(model, module, calib_loader, device)\n",
        "\n",
        "        scale, zero_point = get_quantization_params(module.weight)\n",
        "        quantizer = AdaRoundQuantizer(module.weight, scale)\n",
        "        quantizer.to(device)\n",
        "\n",
        "        final_quantized_weight = train_adaround(\n",
        "            model_layer=module,\n",
        "            input_data=input_data,\n",
        "            orig_output=orig_output,\n",
        "            quantizer=quantizer,\n",
        "            n_iter=2000\n",
        "        )\n",
        "\n",
        "        module.weight.data = final_quantized_weight\n",
        "        print(\"Quantization Complete for this layer!\")\n",
        "\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "KJJkXx63Q9Pv",
        "outputId": "aa8f1030-55d1-4c38-c0fa-2489a2a0ae1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizer Layer : conv1\n",
            "Collecting calibration data for layer: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)...\n",
            "Data Collected! Input shape: torch.Size([1024, 3, 224, 224]), Output shape: torch.Size([1024, 64, 112, 112])\n",
            "Start AdaRound Training...\n",
            "Iter 0: Loss 0.8143, Beta 20.00\n",
            "Iter 500: Loss 0.4458, Beta 15.50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1780324182.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         final_quantized_weight = train_adaround(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mmodel_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1235068692.py\u001b[0m in \u001b[0;36mtrain_adaround\u001b[0;34m(model_layer, input_data, orig_output, quantizer, n_iter)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    954\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         torch._foreach_addcmul_(\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_device_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEfBw3CIS4Ev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}